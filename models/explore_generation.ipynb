{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd063f661667cfff4a21b9f1172704ab3c7d831d3612a4dc528cd9d3281904853c9",
   "display_name": "Python 3.8.5 64-bit ('nlpai': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup, GPT2Model\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "\n",
    "# Gensim for Topic Modelling\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(logger):\n",
    "    \"\"\"\n",
    "    Get device model will be run on (GPU or CPU)\n",
    "    :param logger: Logger object to note the device\n",
    "    :return: device type, num_of_gpus\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logger.info(\"device: {}, n_gpu {}\".format(device, n_gpu))\n",
    "    return device, n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sample_clean_100_15k.csv')\n",
    "# Stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "def stopwords_removing(text):\n",
    "     return ' '.join([word for word in text.split() if not(word in eng_stopwords)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1100 entries, 0 to 1099\nData columns (total 7 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   artist               1100 non-null   object\n 1   song_name            1100 non-null   object\n 2   closest_genre        1100 non-null   object\n 3   lyric                1100 non-null   object\n 4   length_lyric         1100 non-null   int64 \n 5   actual_lyric_length  1100 non-null   int64 \n 6   lyric_processed      1100 non-null   object\ndtypes: int64(2), object(5)\nmemory usage: 60.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "enc2 = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "head = GPT2LMHeadModel.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {\n",
    "        \"additional_special_tokens\": [\n",
    "            '[s:genre]', '[s:song_name]', '[s:lyrics]',\n",
    "            '[e:genre]', '[e:song_name]', '[e:lyrics]'\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'[s:genre]': 50257,\n",
       " '[s:song_name]': 50258,\n",
       " '[s:lyrics]': 50259,\n",
       " '[e:genre]': 50260,\n",
       " '[e:song_name]': 50261,\n",
       " '[e:lyrics]': 50262}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "enc.add_special_tokens(special_tokens_dict)\n",
    "enc.added_tokens_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    simon says glad today surely simon say youl li...\n",
       "1    brian holland lamont dozier edward holland jr ...\n",
       "2    like love see i'm always thinking oh oh oh tre...\n",
       "3    mother mother there's many crying brother brot...\n",
       "4    looking back little nappy headed boy worry chr...\n",
       "Name: lyric_processed, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "\n",
    "data['lyric_processed'] = data.lyric.map(lambda x: re.sub('[(),\\/.!?]', ' ', x))\n",
    "data['lyric_processed'] = data.lyric_processed.map(lambda x: x.lower())\n",
    "data['lyric_processed'] = data.lyric_processed.map(lambda x: stopwords_removing(x))\n",
    "data.lyric_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1100 entries, 0 to 1099\nData columns (total 7 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   artist               1100 non-null   object\n 1   song_name            1100 non-null   object\n 2   closest_genre        1100 non-null   object\n 3   lyric                1100 non-null   object\n 4   length_lyric         1100 non-null   int64 \n 5   actual_lyric_length  1100 non-null   int64 \n 6   lyric_processed      1100 non-null   object\ndtypes: int64(2), object(5)\nmemory usage: 60.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "texts = data_words\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(6,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (12,\n  '0.839*\"closest_genre\" + 0.032*\"artist\" + 0.032*\"song_name\" + 0.032*\"lyric\" '\n  '+ 0.032*\"length_lyric\" + 0.032*\"lyric_processed\"'),\n (14,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (20,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (24,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (10,\n  '0.839*\"song_name\" + 0.032*\"artist\" + 0.032*\"closest_genre\" + 0.032*\"lyric\" '\n  '+ 0.032*\"length_lyric\" + 0.032*\"lyric_processed\"'),\n (22,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (2,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (0,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (4,\n  '0.839*\"artist\" + 0.032*\"song_name\" + 0.032*\"closest_genre\" + 0.032*\"lyric\" '\n  '+ 0.032*\"length_lyric\" + 0.032*\"lyric_processed\"'),\n (5,\n  '0.839*\"lyric_processed\" + 0.032*\"artist\" + 0.032*\"song_name\" + '\n  '0.032*\"closest_genre\" + 0.032*\"lyric\" + 0.032*\"length_lyric\"'),\n (19,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (1,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (3,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (7,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (15,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (17,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (8,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (18,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"'),\n (23,\n  '0.167*\"artist\" + 0.167*\"song_name\" + 0.167*\"closest_genre\" + 0.167*\"lyric\" '\n  '+ 0.167*\"length_lyric\" + 0.167*\"lyric_processed\"')]\n"
     ]
    }
   ],
   "source": [
    "# model LDA\n",
    "num_topics = 25\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = id2word.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create psi matrix\n",
    "psi_matrix = np.zeros((num_topics, enc.vocab_size))\n",
    "lda_topics = lda_model.get_topics()\n",
    "for i in range(len(id2token)):\n",
    "    j = enc.convert_tokens_to_ids(id2token[i])\n",
    "    psi_matrix[:, j] = lda_topics[:, i]\n",
    "pickle.dump(psi_matrix, open('psi_matrix.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create theta matrix\n",
    "num_corpora = len(corpus)\n",
    "theta_matrix = np.zeros((num_corpora, num_topics))\n",
    "for i, c in enumerate(corpus):\n",
    "    for j, p in lda_model.get_document_topics(c):\n",
    "        theta_matrix[i, j] = p\n",
    "pickle.dump(psi_matrix, open('theta_matrix.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = lda_model.show_topics(num_topics=num_topics, formatted=False, num_words=len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all topic tokens\n",
    "tokens_id = [lda_model.get_topic_terms(i, topn=10) for i in range(num_topics)]\n",
    "all_topic_tokens = [[(id2token[i], p) for i, p in tokens_id_topic] for tokens_id_topic in tokens_id]\n",
    "pickle.dump(all_topic_tokens, open('all_topic_tokens.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "artist                                                    Memphis Minnie\n",
       "song_name                    He's In the Ring (Doin' the Same Old Thing)\n",
       "closest_genre                                                      Blues\n",
       "lyric                  If Simon says be glad today\\nI surely do what ...\n",
       "length_lyric                                                         162\n",
       "actual_lyric_length                                                   32\n",
       "lyric_processed        simon says glad today surely simon say youl li...\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# data with token\n",
    "temp = data.iloc[0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = enc.added_tokens_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = []\n",
    "for i, song in data.iterrows():\n",
    "    genre = [ops['[s:genre]']]+enc.encode(song.closest_genre)+[ops['[e:genre]']] \n",
    "    song_name = [ops['[s:song_name]']]+enc.encode(song.song_name)+[ops['[e:song_name]']] \n",
    "    lyrics = [ops['[s:lyrics]']]+enc.encode(song.lyric)+[ops['[e:lyrics]']] \n",
    "    encoded_data.append((genre, song_name, lyrics))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: randomly dropped\n",
    "selected_sentences = []\n",
    "max_input_len = enc.model_max_length\n",
    "for genre, song_name, lyric in encoded_data:\n",
    "    genre_token = list([1] * len(genre))\n",
    "    song_token = list([2] * len(song_name))\n",
    "    lyric_token = list([3] * len(lyric))\n",
    "\n",
    "    if np.random.rand() <= 0.25:\n",
    "        position_ids = list(np.arange(0, len(lyric)))\n",
    "        current_input = {\n",
    "            'token_ids': lyric,\n",
    "            'token_type_ids':lyric_token,\n",
    "            'position_ids':position_ids\n",
    "        }\n",
    "    else:\n",
    "        tokens_subset = []\n",
    "        segment_subset = []\n",
    "\n",
    "        if np.random.rand() > 0.2:\n",
    "            tokens_subset += genre\n",
    "            segment_subset += genre_token\n",
    "\n",
    "        if np.random.rand() > 0.2:\n",
    "            tokens_subset += song_name\n",
    "            segment_subset += song_token\n",
    "\n",
    "        tokens_subset += lyric\n",
    "        segment_subset += lyric_token\n",
    "        position_ids = list(np.arange(0, len(tokens_subset)))\n",
    "\n",
    "        current_input = {\n",
    "            'token_ids': tokens_subset,\n",
    "            'token_type_ids':segment_subset,\n",
    "            'position_ids':position_ids\n",
    "        }\n",
    "    if len(current_input['token_ids']) >= max_input_len:\n",
    "        continue\n",
    "\n",
    "    # Add padding to make the input max_input_len\n",
    "    len_before_padding = len(current_input[\"token_ids\"])\n",
    "    padding = max_input_len - len_before_padding\n",
    "\n",
    "    current_input[\"token_ids\"] += list([0] * padding)\n",
    "    current_input[\"token_type_ids\"] += list([0] * padding)\n",
    "    current_input[\"position_ids\"] += list([0] * padding)\n",
    "\n",
    "    # 4) Language Modelling Labels -> this is input_copy with padding assigned to -1,\n",
    "    #    the position shifting is done in the library code.\n",
    "    lm_labels = np.copy(current_input[\"token_ids\"])\n",
    "    lm_labels[np.where(lm_labels == 0)] = -1\n",
    "\n",
    "    # Attention Mask, 1 = unmasked, 0 = masked\n",
    "    attention_mask = list([1] * len_before_padding) + list([0] * padding)\n",
    "\n",
    "    selected_sentences.append((\n",
    "        current_input[\"token_ids\"], current_input[\"token_type_ids\"], current_input[\"position_ids\"], attention_mask, lm_labels\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_mat = map(list, zip(*selected_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = [torch.tensor(t, device=torch.device(device)).unsqueeze(1) for t in inputs_mat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(*torch_dataset)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "head.to(device)\n",
    "head.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 1\n",
    "max_grad_norm = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/1100 [00:00<?, ?it/s]<ipython-input-155-bfb2fddbd7ae>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_condition = torch.where(lm_labels==-1, torch.tensor(0, dtype=torch.int32).to(device), torch.tensor(lm_labels))\n",
      "Training:   0%|          | 0/1100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'topic_word_matrix'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-bfb2fddbd7ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtok_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtok_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matt_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlm_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlabel_condition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlm_labels\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlm_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     outputs = head(\n\u001b[0m\u001b[0;32m      6\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtok_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mpast_key_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\nlpai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'topic_word_matrix'"
     ]
    }
   ],
   "source": [
    "past = None\n",
    "for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "    tok_ids, tok_type_ids, pos_ids, att_mask, lm_labels = batch\n",
    "    label_condition = torch.where(lm_labels==-1, torch.tensor(0, dtype=torch.int32).to(device), torch.tensor(lm_labels))\n",
    "    outputs = head(\n",
    "                    input_ids=tok_ids, \n",
    "                    past_key_values=past, \n",
    "                    attention_mask=att_mask, \n",
    "                    token_type_ids=tok_type_ids,\n",
    "                    position_ids=pos_ids, \n",
    "                    topic_word_matrix=topic_words,\n",
    "                    labels=label_condition\n",
    "                )\n",
    "    loss = outputs[0]\n",
    "    # predicted_scores = outputs[1]\n",
    "    # past = outputs[2]\n",
    "\n",
    "    # Log the loss to TensorBoardX\n",
    "    global_step = (1 * len(train_data_loader)) + (step + 1)\n",
    "\n",
    "    # Normalise the loss (Simulates average of a batch)\n",
    "    loss = loss / args.gradient_accumulation_steps\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    if (step + 1) % gradient_accumulation_steps == 0:\n",
    "        torch.nn.utils.clip_grad_norm_(head.parameters(), args.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}