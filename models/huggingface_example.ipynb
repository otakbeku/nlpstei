{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd063f661667cfff4a21b9f1172704ab3c7d831d3612a4dc528cd9d3281904853c9",
   "display_name": "Python 3.8.5 64-bit ('nlpai': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reproducability\n",
    "SEED = 34\n",
    "\n",
    "#maximum number of words in output text\n",
    "MAX_LEN = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = \"I don't know about you, but there's only one thing I want to do after a long day of work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 1.04M/1.04M [00:11<00:00, 87.0kB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:05<00:00, 77.8kB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:08<00:00, 158kB/s]\n",
      "Downloading: 100%|██████████| 665/665 [00:00<00:00, 444kB/s]\n",
      "Downloading: 100%|██████████| 498M/498M [32:04<00:00, 259kB/s]\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Model: \"tfgp_t2lm_head_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "transformer (TFGPT2MainLayer multiple                  124439808 \n",
      "=================================================================\n",
      "Total params: 124,439,808\n",
      "Trainable params: 124,439,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#get transformers\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# #get large GPT2 tokenizer and GPT2 model\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "# GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#view model parameters\n",
    "GPT2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output:\n----------------------------------------------------------------------------------------------------\nI don't know about you, but there's only one thing I want to do after a long day of work. I want to get out of here. I want to get out of here. I want to get out of here. I want to get out of here. I want to get out of here. I want to get out of\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nOutput:\n----------------------------------------------------------------------------------------------------\n0: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to get out of bed.\"\n\n\"I'm not going to lie to you,\" she said. \"It's not like I've ever done anything like this before. It's just a matter of time.\"\n1: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to get out of bed.\"\n\n\"I'm not going to lie to you,\" she said. \"It's not like I've ever done anything like this before. It's just a matter of time before\n2: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to get out of bed.\"\n\n\"I'm not going to lie to you,\" she said. \"It's not like I've ever done anything like this before. It's just a matter of time.\n3: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to get out of bed.\"\n\n\"I'm not going to lie to you,\" she said. \"It's not like I've ever done anything like this before. It's just a matter of getting out\n4: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to get out of bed.\"\n\n\"I'm not going to lie to you,\" she said. \"It's not like I've ever done anything like this before. It's just a matter of getting back\n"
     ]
    }
   ],
   "source": [
    "# Beam search with N-Gram Penalities\n",
    "# set return_num_sequences > 1\n",
    "beam_outputs = GPT2.generate(\n",
    "    input_ids, \n",
    "    max_length = MAX_LEN, \n",
    "    num_beams = 5, \n",
    "    no_repeat_ngram_size = 2, \n",
    "    num_return_sequences = 5, \n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "print('')\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "# now we have 3 output sequences\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output:\n----------------------------------------------------------------------------------------------------\nI don't know about you, but there's only one thing I want to do after a long day of work that I can't sit through all day. I need to get up and go to work!\" Everyone must have heard that. People would have from school that sounded like a great idea. \"Be like that, Joe. Someone is going\n"
     ]
    }
   ],
   "source": [
    "# Basic sampling\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 0, \n",
    "                             temperature = 0.8\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output:\n----------------------------------------------------------------------------------------------------\nI don't know about you, but there's only one thing I want to do after a long day of work!\"\n\nAfter five hours of questioning from the two assistants, they managed to find him and they finally decided to take him home. It turned out that when the train went over, Mr. Smith was nowhere to be found. ...\n"
     ]
    }
   ],
   "source": [
    "# Top-K sampling\n",
    "#sample from only top_k most likely words\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output:\n----------------------------------------------------------------------------------------------------\nI don't know about you, but there's only one thing I want to do after a long day of work and being happy: eat like a champ. Why wouldn't I have done this? I'd be better off not worrying about anything else.\n\n\"If you don't want to eat well or even if you want to have long ...\n"
     ]
    }
   ],
   "source": [
    "# Top-P sampling\n",
    "#sample only from 80% most likely words\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_p = 0.8, \n",
    "                             top_k = 0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output:\n----------------------------------------------------------------------------------------------------\n0: I don't know about you, but there's only one thing I want to do after a long day of work. To have your best and best friends to play with.\"\n\nWhile most of the players who had been invited to watch the match were already signed to clubs for the season, he said they would join him for the end of the season.\n\nHe said: \"I would like to play again next season, but only if I am selected.\n\n\"I want to go back to a place where I can train and do things that I enjoy.\n\n\"There are so many great players here. I don't know if I will come back to Tottenham or if...\n\n1: I don't know about you, but there's only one thing I want to do after a long day of work and I'm about to go into labor.\"...\n\n2: I don't know about you, but there's only one thing I want to do after a long day of work: get an hour of sleep every night. It doesn't happen that often. But I've heard about it a lot.\n\nHow much is it?\n\nWell, I don't know, but it's like an hour of sleep every night for me. And the time it takes me to get through this is very short. The first day of work, my body is working at about the same rate as an hour of sleep every night.\n\nYou said there's no time to be happy. Do you think it's fair to say you get a better sleep each...\n\n3: I don't know about you, but there's only one thing I want to do after a long day of work. You should be able to finish with a very short workday and you're all done.\"\n\nA spokeswoman for the United Kingdom Government declined to comment on whether she was aware of any specific employment plans for her.\n\n\"Ms. Cameron had no further comment at this time,\" said her spokesman. \"The Minister was happy to confirm that we were providing our own guidance regarding the status of people who need a workday. We are not seeking employment in this situation, and will not seek to change employment arrangements.\"\n\nIt follows an outcry by the Scottish Conservatives over plans...\n\n4: I don't know about you, but there's only one thing I want to do after a long day of work. I want to get to work.\"\n\nAnd then there's the issue of where the family is going to go if they aren't able to make it.\n\n\"I'm not sure how long it will be, but I'm really, really looking forward to it,\" said B.L. \"I'm really looking forward to the kids.\"\n\nBenson said he's not looking forward to having a second child.\n\n\"I'm looking forward to getting the children out of here and getting the best care for them,\" he said.\n\nBut it...\n\n"
     ]
    }
   ],
   "source": [
    "# Top-K and Top-P Sampling\n",
    "#combine both sampling techniques\n",
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = 2*MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .7,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85, \n",
    "                              num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wed Apr 14 13:45:56 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 461.72       Driver Version: 461.72       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 2060   WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   62C    P8    13W /  N/A |    653MiB /  6144MiB |      9%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1092    C+G   Insufficient Permissions        N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on built-in function bmm:\n\nbmm(...)\n    bmm(input, mat2, *, deterministic=False, out=None) -> Tensor\n    \n    Performs a batch matrix-matrix product of matrices stored in :attr:`input`\n    and :attr:`mat2`.\n    \n    :attr:`input` and :attr:`mat2` must be 3-D tensors each containing\n    the same number of matrices.\n    \n    If :attr:`input` is a :math:`(b \\times n \\times m)` tensor, :attr:`mat2` is a\n    :math:`(b \\times m \\times p)` tensor, :attr:`out` will be a\n    :math:`(b \\times n \\times p)` tensor.\n    \n    .. math::\n        \\text{out}_i = \\text{input}_i \\mathbin{@} \\text{mat2}_i\n    \n    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n    \n    .. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.\n              For broadcasting matrix products, see :func:`torch.matmul`.\n    \n    Args:\n        input (Tensor): the first batch of matrices to be multiplied\n        mat2 (Tensor): the second batch of matrices to be multiplied\n    \n    Keyword Args:\n        deterministic (bool, optional): flag to choose between a faster non-deterministic\n                                        calculation, or a slower deterministic calculation.\n                                        This argument is only available for sparse-dense CUDA bmm.\n                                        Default: ``False``\n        out (Tensor, optional): the output tensor.\n    \n    Example::\n    \n        >>> input = torch.randn(10, 3, 4)\n        >>> mat2 = torch.randn(10, 4, 5)\n        >>> res = torch.bmm(input, mat2)\n        >>> res.size()\n        torch.Size([10, 3, 5])\n\n"
     ]
    }
   ],
   "source": [
    "help(torch.bmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt1, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output:\n----------------------------------------------------------------------------------------------------\n0: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\"We have been studying the language of humans for thousands of years. We discovered that this language, which we call Proto-Siberian, may be used in a variety of languages as well, as well as different cultures, including people of different ethnic backgrounds,\" said lead researcher Dr. Aja Jörg Jensen, from the University of Colorado and a member of the U.S. Geological Survey. \"We're very excited to have found out about this discovery and that's a really exciting discovery.\"\n...\n\n"
     ]
    }
   ],
   "source": [
    "# Benchmark Prompts\n",
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake news generated from GPT2\n",
    "prompt2 = 'Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt2, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output:\n----------------------------------------------------------------------------------------------------\n0: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.\n\n\nThe singer was spotted on a flight to Chicago to be photographed at a Target in St. Paul, Minnesota.\n\n\nScroll down for video\n\n\nSavage: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today today\n\n\nRidiculous: Miley's mother told TMZ she heard her daughter's rant and then went to the Target and ordered the food\n\nMiley Cyrus on the 'Glee' hit the set with a man in his 20s wearing a T-shirt and jeans with a 'Glee' logo at the Target in St. Paul today.\n\n\n...\n\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85\n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another fake news\n",
    "prompt3 = 'Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt3, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output:\n----------------------------------------------------------------------------------------------------\n0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n\nAbandoned by the orcs, he sent his minions across the vast wilderness. On his way, they reached a small rock formation and encountered a group of men. One of them made a run for it but they were unable to find it. A second, slightly larger orc led by Rhaegar attacked the group. He was easily defeated and Rhaegar returned to fight with his men, but was beaten by both of them.\n\nIn a fit of rage, he threw his armor over his head and cursed the men for being so foolish. When the men asked why they were not killed, he pointed at the orcs and...\n\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake homework assignemnts\n",
    "prompt4 = \"For today’s homework assignment, please describe the reasons for the US Civil War.\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt4, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output:\n----------------------------------------------------------------------------------------------------\n0: For today’s homework assignment, please describe the reasons for the US Civil War.\n\n\n1. The Civil War was a \"civil war\" that started on April 21st, 1865.\n\n\n2. The US government tried to destroy the Confederacy by creating a new government, the Confederacy of Independent States. The new government was called the \"Citizens of the United States\" (or \"DSA\"), a title which would have meant all slaves were white. It was then that the \"DSA\" became known as the Confederacy of Independent States. The first African American president was named Abraham Lincoln.\n\n\n3. In January of 1865, Confederate soldiers took the Civil War to the battlefield in the eastern part of the South, along...\n\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}