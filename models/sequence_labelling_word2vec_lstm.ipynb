{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Sequence Labelling using Word2Vec LSTM\n\nPada notebook kami mengimplementasikan sequence labelling menggunakan LSTM. Pertama yang perlu dilakukan adalah menginstal library dan mengunduh data yang penting digunakan pada eksperimen ini. Kami menggunakan data dari lirik lagu pada eksperimen sebelumnya dan mengambil sebanyak 1080 kalimat."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install stanza\n!git clone https://github.com/otakbeku/nlpstei.git\n!pip install wandb --upgrade","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kami menggunakan wandb sebagai alat bantu untuk memantau proses berjalannya setiap iterasi"},{"metadata":{"trusted":true},"cell_type":"code","source":"import wandb\n\nwandb.login()\nwandb.init(project=\"sequence labelling\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport pandas as pd\nimport heapq\nimport pprint\n\nfrom nltk.tokenize import wordpunct_tokenize, blankline_tokenize, line_tokenize, word_tokenize\nfrom itertools import combinations\nfrom nltk.corpus import stopwords\nfrom time import time \nfrom gensim.models import Word2Vec, KeyedVectors\nimport multiprocessing\nfrom collections import namedtuple\n\n# # tensorflow\n# import tensorflow as tf\n# from tensorflow import keras\n# from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\n# Pytorch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n\n# stanza\nimport stanza as st\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mengunduh vocabulary untuk stanza\n\nKami menggunakan stanza untuk memberikan label pada masing-masing token."},{"metadata":{"trusted":true},"cell_type":"code","source":"st.download('en')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Glove sebagai Embedding matrix\n\nKami menggunakan pretrained model untuk embedding matrix. Pada baris terakhir terdapat keterangan bahwa kami menambahkan array baru pada matrix yang digunakan sebagai padding dan unknown word yang tidak terdapat pada vocab yang diberikan"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pretrained word2vec\nimport gensim.downloader as api\ncorpus = api.load('glove-wiki-gigaword-50', return_path=True)\npretrainedwvmodel = KeyedVectors.load_word2vec_format(corpus)\nembedding_matrix = pretrainedwvmodel.wv.vectors\nembedding_matrix = np.append(embedding_matrix, np.zeros((1,50)), axis=0) # Padding\nembedding_matrix = np.append(embedding_matrix, np.zeros((1,50)), axis=0)\nembedding_matrix = np.append(embedding_matrix, np.zeros((1,50)), axis=0) # Unknown word","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tag as Class\n\nAda 38 tag yang kami gunakan pada eksperimen ini. Tag ini didapat dari Penn treebank POS tagger. Untuk tag yang tidak diketahui atau tidak terdapat pada vocab maka akan digantikan dengan `<UNK>`. Sedangkan untuk padding menggunakan `<PAD>`"},{"metadata":{"trusted":true},"cell_type":"code","source":"TAG2CLASS = {\n    '<PAD>': 0,\n    'CC': 1,\n    'CD': 2,\n    'DT': 3,\n    'EX': 4,\n    'FW': 5,\n    'IN': 6,\n    'JJ': 7,\n    'JJR': 8,\n    'JJS': 9,\n    'LS': 10,\n    'MD': 11,\n    'NN': 12,\n    'NNS': 13,\n    'NNP': 14,\n    'NNPS': 15,\n    'PDT': 16,\n    'POS': 17,\n    'PRP': 18,\n    'PRP$': 19,\n    'RB': 20,\n    'RBR': 21,\n    'RBS': 22,\n    'RP': 23,\n    'SYM': 24,\n    'TO': 25,\n    'UH': 26,\n    'VB': 27,\n    'VBD': 28,\n    'VBG': 29,\n    'VBN': 30,\n    'VBP': 31,\n    'VBZ': 32,\n    'WDT': 33,\n    'WP': 34,\n    'WP$': 35,\n    'WRB': 36,\n    '-RRB-': 37,\n    '-LRB-':38,\n        '<UNK>': 0,\n    \n}\npos_tagger = st.Pipeline(lang='en', use_gpu=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mapping data dengan label\n\nKami membuat Dataset sendiri yang khusus dipakai pada eksperimen ini. Secara sederhana, kelas DataMapper1 ini akan menghasilkan data dengan label berupa sequence. Data dalam bentuk kalimat yang sudah ditokenisasi dan label berupa POS tagger dari masing-masing token."},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataMapper1(Dataset):\n    def __init__(self, sentence_lyrics, wvmodel, sequence_len):\n        self.sents = sentence_lyrics\n        self.sequence_len = sequence_len\n        self.model = wvmodel\n\n    def __len__(self):\n        return len(self.sents)\n\n    def __getitem__(self, idx):\n        doc = pos_tagger(self.sents[idx])\n        xl = []\n        yl = []\n        seq = np.zeros(self.sequence_len, dtype=np.int64)\n        yseq = np.zeros(self.sequence_len, dtype=np.int64)\n        for k in doc.sentences[0].words:\n            if (self.model.wv.vocab.get(k.text) is None):\n                xl.append(400002)\n                yl.append(TAG2CLASS.get('<UNK>'))\n                continue\n            xl.append(self.model.wv.vocab.get(k.text).index)\n            yl.append(TAG2CLASS.get(k.xpos, 0))\n        seq[:len(xl)] = xl[:self.sequence_len]\n        yseq[:len(yl)] = yl[:self.sequence_len]\n        return seq, yseq\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ini adalah proses seleksi data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('./nlpstei/models/Sentences_15klyrics_mls_20.csv')\ntrain_data = data.sent[:800].to_numpy()\nval_random = np.random.choice(data[:800].to_numpy().flatten(), 80)\nval_data = np.append(val_random, data.sent[1001:1081].to_numpy())\ntest_data = data.sent[800:1001].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = DataMapper1(train_data, pretrainedwvmodel, 20)\nval_set = DataMapper1(val_data, pretrainedwvmodel, 20)\ntest_set = DataMapper1(test_data, pretrainedwvmodel, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loader_training = DataLoader(training_set, batch_size=16)\nloader_val = DataLoader(training_set, batch_size=16)\nloader_test = DataLoader(test_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model LSTM\n\nKami menggunakan model LSTM sederhana dengan hanya menggunakan 1 layer LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Simple_Sequence_LSTMver2(nn.Module):\n\n    def __init__(self, args):\n        super(Simple_Sequence_LSTMver2, self).__init__()\n        # Hyperparameters\n        self.hidden_dim = args.hidden_dim\n        self.LSTM_layers = args.lstm_layers\n        self.embedding_matrix = args.embedding_matrix.cuda()\n        self.target_size = args.target_size\n        self.tag_class_size = args.class_number\n\n        self.word_embeddings = nn.Embedding.from_pretrained(\n            self.embedding_matrix, padding_idx=args.padding_idx, freeze=True)\n\n        # The LSTM takes word embeddings as inputs, and outputs hidden states\n        # with dimensionality hidden_dim.\n        self.lstm = nn.LSTM(self.hidden_dim, self.hidden_dim)\n\n        # The linear layer that maps from hidden state space to tag space\n        self.hidden2tag = nn.Linear(self.hidden_dim, self.tag_class_size)\n\n    def forward(self, sentence):\n        # # Hidden and cell state definion\n        # h = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim)).cuda()\n        # c = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim)).cuda()\n\n        # # Initialization fo hidden and cell states\n        # torch.nn.init.xavier_normal_(h)\n        # torch.nn.init.xavier_normal_(c)\n\n        embeds = self.word_embeddings(sentence)\n        # lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1),(h, c))\n        # print(embeds.view(len(sentence), 1, -1).shape)\n        lstm_out, _ = self.lstm(embeds)\n        tag_space = self.hidden2tag(lstm_out)\n        # print(lstm_out.view(len(sentence), -1).shape)\n        # tag_space = tag_space.view(len(sentence), self.tag_class_size)\n        tag_scores = torch.sigmoid_(tag_space)\n        return tag_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = torch.FloatTensor(embedding_matrix)\ntrain_on_gpu = torch.cuda.is_available()\nlstm_dict = {\n    # 'batch_size':8,\n    'hidden_dim': embedding_matrix.shape[1],\n    'lstm_layers':3,\n    # 'input_size':embedding_matrix.shape[0],\n    'padding_idx': 400001,\n    'target_size': 20,\n    'class_number': 40,\n    'embedding_matrix': embedding_matrix\n}\nlstm_args = namedtuple('lstm_args', lstm_dict.keys())(**lstm_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Simple_Sequence_LSTMver2(lstm_args).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def categorical_accuracy(preds, y, tag_pad_idx=0):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    \"\"\"\n    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n    non_pad_elements = (y != tag_pad_idx).nonzero()\n    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements]).cuda()\n    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = filter(lambda p: p.requires_grad, model.parameters())\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01,momentum=0.9,weight_decay=0.0001)\nloss_function = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validation_metrics (model, valid_dl):\n    loss_function = nn.CrossEntropyLoss()\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    acc_total = 0.0\n    for x, y in valid_dl:\n        x = x.cuda()\n        y = y.cuda()\n        y_hat = model(x)\n        y_pred_2 = y_hat.view(-1, y_hat.shape[-1])\n        y_2 = y.view(-1)\n        loss = loss_function(y_pred_2, y_2)\n        pred = torch.max(y_hat, 0)[1]\n        correct += categorical_accuracy(y_pred_2, y_2).item()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n\n    return sum_loss/total, correct/total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wandb.watch(model, loss_function, log=\"all\", log_freq=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Proses Training\n\n\nProses training dilakukan sebanyak 10 epoch dan pada setiap iterasi jika ditemukan hasil validasi yang bagus, maka model tersebut akan disimpan"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10\nvalidation_loss_min = np.inf\nfor i in tqdm(range(epochs)):\n    model.train()\n    sum_loss = 0.0\n    total = 0\n    for x, y in loader_training:\n        x = torch.tensor(x).to(torch.long).cuda()\n        y_pred = model(x)\n        y = torch.tensor(y).to(torch.long).cuda()\n        \n        optimizer.zero_grad()\n        y_pred_max = torch.argmax(y_pred, dim=2)\n        y_pred_2 = y_pred.view(-1, y_pred.shape[-1])\n        y_2 = y.view(-1)\n        loss = loss_function(y_pred_2, y_2)\n        loss.backward()\n        optimizer.step()\n        sum_loss += loss.item()*y_2.shape[0]\n        total += y.shape[0]\n        acc = categorical_accuracy(y_pred_2, y_2)\n    val_loss, val_acc = validation_metrics(model, loader_val)\n#     wandb.log(f'Epoch: {i}\\tTraining loss: {sum_loss}\\tValidation loss: {val_loss}')\n    wandb.log({'training_loss':sum_loss, 'validation_loss':val_loss})\n    print(f'Epoch: {i}\\tTraining loss: {sum_loss}\\tValidation loss: {val_loss}')\n    if val_loss <= validation_loss_min:\n        print('\\t\\tValidation loss-nya lebih kecil!')\n#         wandb.log('\\t\\tValidation loss-nya lebih kecil!')\n        torch.save(model.state_dict(),\"../working/model_seq_lyrics_best3.pth\")\n        validation_loss_min = val_loss\n    if i % 5 == 1:\n        print(\"train loss %.3f, val loss %.3f, val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n#         wandb.log(\"train loss %.3f, val loss %.3f, val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n        train_loss_epoch = sum_loss/total\n        val_loss_epoch = val_loss\n        val_acc_epoch = val_acc\n        wandb.log({'train_loss_mod': train_loss_epoch, 'val_loss_epoch': val_loss, 'val_accuracy': val_acc})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cloud Storage\nfrom google.cloud import storage\nstorage_client = storage.Client(project='239480140419')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bucket = storage_client.bucket('song_lyrics')\nblob = bucket.blob(\"model_seq_lyrics_best3.pth\")\nblob.upload_from_filename(\"../working/model_seq_lyrics_best3.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_loss, val_acc = validation_metrics(model, loader_val)\nprint(\"val loss %.3f, val accuracy %.3f\" % (val_loss, val_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = validation_metrics(model, loader_test)\nprint(\"test loss %.3f, test accuracy %.3f\" % (test_loss, test_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}